#Aplicando o One-Label enconder nos atributos categoricos binarios
data$gender<- ifelse(data$gender == "female", 0, 1)
data$Winter.Training.Camp<- ifelse(data$Winter.Training.Camp == "none", 0, 1)
data$Pro.level <- ifelse(data$Pro.level == "Continental", 0, 1)
#Aplicando o One-Hot Enconder nos atrbutos categoricos não binarios
#data <- data[,-c(9,2,5,8,10)]
encoded_df <- dummyVars("~.", data = data)
data <- data.frame(predict(encoded_df, newdata = data))
#1 - Estude a capacidade preditiva relativamente ao atributo “Pro_level”
#usando os seguintes métodos:
#divisão do dataset em treino e validação
sample = sample(c(TRUE,FALSE), nrow(data), replace = TRUE, prob = c(0.7,0.3))
prolevel.train = data[sample,]
prolevel.test = data[!sample,]
#árvore de decisão;
#A arvore de decisão tem melhores resultados sem essas colunas
#data <- data[,-c(9,2,5,8,10)]
tree.model <- rpart(Pro.level ~ . , data = prolevel.train, method = "class")
fancyRpartPlot(tree.model)
rpart.plot(tree.model, digits = 3, fallen.leaves = TRUE)
tree.pred = predict(tree.model, prolevel.test, type="class")
parse_results <- function(m.conf) {
accuracy <- 100*round((m.conf[1,1]+m.conf[2,2])/sum(m.conf),4)
recall= m.conf[1,1]/(m.conf[1,1]+m.conf[1,2])
precision = m.conf[1,1]/(m.conf[1,1]+m.conf[2,1])
f1=(2*precision * recall)/(precision+recall)
message("accuracy: ", accuracy, "%")
message("Recall: ", recall)
message("precision: ", precision)
message("F1: ", f1)
my_list <- list("F1" = f1, "precision" = precision, "recall" = recall, "accuracy"=accuracy)
return(my_list)
}
m.conf<-table(prolevel.test$Pro.level,tree.pred)
parse_results(m.conf)
#KNN
prolevel.train$Pro.level <- factor(prolevel.train$Pro.level)
cv <- trainControl(method = "cv",
number = 10,
search = "grid")
tuned_model <- train(Pro.level ~ .,
data = prolevel.train,
method = "knn",
trControl = cv,
tuneGrid = expand.grid(k = 1:10))
best_k <- tuned_model$bestTune$k
final_model <- knn(train = prolevel.train,
test = prolevel.test,
cl = prolevel.train$Pro.level,
k = best_k)
confusion_matrix <- table(final_model, prolevel.test$Pro.level)
parse_results(confusion_matrix)
library(caret)
library(lubridate)
library(rpart)
library(rpart.plot)
library(rattle)
library(class)
set.seed(123)
setwd("C:/Users/manu0/Desktop/RESTO/ANADI/TP2/data")
data <- read.csv("ciclismo.csv")
#Transformando as datas em idades
ages <- interval(as.Date(data$dob), today()) %/% years(1)
data$age <- ages
#Omitindo NA's
data <- na.omit(data)
#Retirando a coluna dos Id's e das datas de nascimento
data <- data[,-c(1, 10)]
#Função de normalização
min_max <- function(data_aux) {
min_value <- min(data_aux)
max_value <- max(data_aux)
result <- (data_aux - min_value)/(max_value - min_value)
return(result)
}
#Normalização dos atributos
data$age <- min_max(data$age)
data$altitude_results <- min_max(data$altitude)
data$vo2_results <- min_max(data$vo2_results)
data$hr_results <- min_max(data$hr_results)
#Aplicando o One-Label enconder nos atributos categoricos binarios
data$gender<- ifelse(data$gender == "female", 0, 1)
data$Winter.Training.Camp<- ifelse(data$Winter.Training.Camp == "none", 0, 1)
data$Pro.level <- ifelse(data$Pro.level == "Continental", 0, 1)
#Aplicando o One-Hot Enconder nos atrbutos categoricos não binarios
data <- data[,-c(9)]
encoded_df <- dummyVars("~.", data = data)
data <- data.frame(predict(encoded_df, newdata = data))
#1 - Estude a capacidade preditiva relativamente ao atributo “Pro_level”
#usando os seguintes métodos:
#divisão do dataset em treino e validação
sample = sample(c(TRUE,FALSE), nrow(data), replace = TRUE, prob = c(0.7,0.3))
prolevel.train = data[sample,]
prolevel.test = data[!sample,]
#árvore de decisão;
#A arvore de decisão tem melhores resultados sem essas colunas
#data <- data[,-c(9,2,5,8,10)]
tree.model <- rpart(Pro.level ~ . , data = prolevel.train, method = "class")
fancyRpartPlot(tree.model)
rpart.plot(tree.model, digits = 3, fallen.leaves = TRUE)
tree.pred = predict(tree.model, prolevel.test, type="class")
parse_results <- function(m.conf) {
accuracy <- 100*round((m.conf[1,1]+m.conf[2,2])/sum(m.conf),4)
recall= m.conf[1,1]/(m.conf[1,1]+m.conf[1,2])
precision = m.conf[1,1]/(m.conf[1,1]+m.conf[2,1])
f1=(2*precision * recall)/(precision+recall)
message("accuracy: ", accuracy, "%")
message("Recall: ", recall)
message("precision: ", precision)
message("F1: ", f1)
my_list <- list("F1" = f1, "precision" = precision, "recall" = recall, "accuracy"=accuracy)
return(my_list)
}
m.conf<-table(prolevel.test$Pro.level,tree.pred)
parse_results(m.conf)
#KNN
prolevel.train$Pro.level <- factor(prolevel.train$Pro.level)
cv <- trainControl(method = "cv",
number = 10,
search = "grid")
tuned_model <- train(Pro.level ~ .,
data = prolevel.train,
method = "knn",
trControl = cv,
tuneGrid = expand.grid(k = 1:10))
best_k <- tuned_model$bestTune$k
final_model <- knn(train = prolevel.train,
test = prolevel.test,
cl = prolevel.train$Pro.level,
k = best_k)
confusion_matrix <- table(final_model, prolevel.test$Pro.level)
parse_results(confusion_matrix)
library(caret)
library(lubridate)
library(rpart)
library(rpart.plot)
library(rattle)
library(class)
set.seed(123)
setwd("C:/Users/manu0/Desktop/RESTO/ANADI/TP2/data")
data <- read.csv("ciclismo.csv")
#Transformando as datas em idades
ages <- interval(as.Date(data$dob), today()) %/% years(1)
data$age <- ages
#Omitindo NA's
data <- na.omit(data)
#Retirando a coluna dos Id's e das datas de nascimento
data <- data[,-c(1, 10)]
#Função de normalização
min_max <- function(data_aux) {
min_value <- min(data_aux)
max_value <- max(data_aux)
result <- (data_aux - min_value)/(max_value - min_value)
return(result)
}
#Normalização dos atributos
data$age <- min_max(data$age)
data$altitude_results <- min_max(data$altitude)
data$vo2_results <- min_max(data$vo2_results)
data$hr_results <- min_max(data$hr_results)
#Aplicando o One-Label enconder nos atributos categoricos binarios
data$gender<- ifelse(data$gender == "female", 0, 1)
data$Winter.Training.Camp<- ifelse(data$Winter.Training.Camp == "none", 0, 1)
data$Pro.level <- ifelse(data$Pro.level == "Continental", 0, 1)
#Aplicando o One-Hot Enconder nos atrbutos categoricos não binarios
data <- data[,-c(9,2)]
encoded_df <- dummyVars("~.", data = data)
data <- data.frame(predict(encoded_df, newdata = data))
#1 - Estude a capacidade preditiva relativamente ao atributo “Pro_level”
#usando os seguintes métodos:
#divisão do dataset em treino e validação
sample = sample(c(TRUE,FALSE), nrow(data), replace = TRUE, prob = c(0.7,0.3))
prolevel.train = data[sample,]
prolevel.test = data[!sample,]
#árvore de decisão;
#A arvore de decisão tem melhores resultados sem essas colunas
#data <- data[,-c(9,2,5,8,10)]
tree.model <- rpart(Pro.level ~ . , data = prolevel.train, method = "class")
fancyRpartPlot(tree.model)
rpart.plot(tree.model, digits = 3, fallen.leaves = TRUE)
tree.pred = predict(tree.model, prolevel.test, type="class")
parse_results <- function(m.conf) {
accuracy <- 100*round((m.conf[1,1]+m.conf[2,2])/sum(m.conf),4)
recall= m.conf[1,1]/(m.conf[1,1]+m.conf[1,2])
precision = m.conf[1,1]/(m.conf[1,1]+m.conf[2,1])
f1=(2*precision * recall)/(precision+recall)
message("accuracy: ", accuracy, "%")
message("Recall: ", recall)
message("precision: ", precision)
message("F1: ", f1)
my_list <- list("F1" = f1, "precision" = precision, "recall" = recall, "accuracy"=accuracy)
return(my_list)
}
m.conf<-table(prolevel.test$Pro.level,tree.pred)
parse_results(m.conf)
#KNN
prolevel.train$Pro.level <- factor(prolevel.train$Pro.level)
cv <- trainControl(method = "cv",
number = 10,
search = "grid")
tuned_model <- train(Pro.level ~ .,
data = prolevel.train,
method = "knn",
trControl = cv,
tuneGrid = expand.grid(k = 1:10))
best_k <- tuned_model$bestTune$k
final_model <- knn(train = prolevel.train,
test = prolevel.test,
cl = prolevel.train$Pro.level,
k = best_k)
confusion_matrix <- table(final_model, prolevel.test$Pro.level)
parse_results(confusion_matrix)
library(caret)
library(lubridate)
library(rpart)
library(rpart.plot)
library(rattle)
library(class)
set.seed(123)
setwd("C:/Users/manu0/Desktop/RESTO/ANADI/TP2/data")
data <- read.csv("ciclismo.csv")
#Transformando as datas em idades
ages <- interval(as.Date(data$dob), today()) %/% years(1)
data$age <- ages
#Omitindo NA's
data <- na.omit(data)
#Retirando a coluna dos Id's e das datas de nascimento
data <- data[,-c(1, 10)]
prolevel.train$Pro.level <- factor(prolevel.train$Pro.level)
cv <- trainControl(method = "cv",
number = 10,
search = "grid")
tuned_model <- train(Pro.level ~ .,
data = prolevel.train,
method = "knn",
trControl = cv,
tuneGrid = expand.grid(k = 1:10))
best_k <- tuned_model$bestTune$k
final_model <- knn(train = prolevel.train,
test = prolevel.test,
cl = prolevel.train$Pro.level,
k = best_k)
confusion_matrix <- table(final_model, prolevel.test$Pro.level)
parse_results(confusion_matrix)
sample = sample(c(TRUE,FALSE), nrow(data), replace = TRUE, prob = c(0.7,0.3))
prolevel.train = data[sample,]
prolevel.test = data[!sample,]
prolevel.train$Pro.level <- factor(prolevel.train$Pro.level)
cv <- trainControl(method = "cv",
number = 10,
search = "grid")
tuned_model <- train(Pro.level ~ .,
data = prolevel.train,
method = "knn",
trControl = cv,
tuneGrid = expand.grid(k = 1:10))
best_k <- tuned_model$bestTune$k
final_model <- knn(train = prolevel.train,
test = prolevel.test,
cl = prolevel.train$Pro.level,
k = best_k)
library(caret)
library(lubridate)
library(rpart)
library(rpart.plot)
library(rattle)
library(class)
set.seed(123)
setwd("C:/Users/manu0/Desktop/RESTO/ANADI/TP2/data")
data <- read.csv("ciclismo.csv")
#Transformando as datas em idades
ages <- interval(as.Date(data$dob), today()) %/% years(1)
data$age <- ages
#Omitindo NA's
data <- na.omit(data)
#Retirando a coluna dos Id's e das datas de nascimento
data <- data[,-c(1, 10)]
#Função de normalização
min_max <- function(data_aux) {
min_value <- min(data_aux)
max_value <- max(data_aux)
result <- (data_aux - min_value)/(max_value - min_value)
return(result)
}
#Normalização dos atributos
data$age <- min_max(data$age)
data$altitude_results <- min_max(data$altitude)
data$vo2_results <- min_max(data$vo2_results)
data$hr_results <- min_max(data$hr_results)
#Aplicando o One-Label enconder nos atributos categoricos binarios
data$gender<- ifelse(data$gender == "female", 0, 1)
data$Winter.Training.Camp<- ifelse(data$Winter.Training.Camp == "none", 0, 1)
data$Pro.level <- ifelse(data$Pro.level == "Continental", 0, 1)
#Aplicando o One-Hot Enconder nos atrbutos categoricos não binarios
encoded_df <- dummyVars("~.", data = data)
data <- data.frame(predict(encoded_df, newdata = data))
#1 - Estude a capacidade preditiva relativamente ao atributo “Pro_level”
#usando os seguintes métodos:
#divisão do dataset em treino e validação
sample = sample(c(TRUE,FALSE), nrow(data), replace = TRUE, prob = c(0.7,0.3))
prolevel.train = data[sample,]
prolevel.test = data[!sample,]
#árvore de decisão;
#A arvore de decisão tem melhores resultados sem essas colunas
#data <- data[,-c(9,2,5,8,10)]
tree.model <- rpart(Pro.level ~ . , data = prolevel.train, method = "class")
fancyRpartPlot(tree.model)
rpart.plot(tree.model, digits = 3, fallen.leaves = TRUE)
tree.pred = predict(tree.model, prolevel.test, type="class")
parse_results <- function(m.conf) {
accuracy <- 100*round((m.conf[1,1]+m.conf[2,2])/sum(m.conf),4)
recall= m.conf[1,1]/(m.conf[1,1]+m.conf[1,2])
precision = m.conf[1,1]/(m.conf[1,1]+m.conf[2,1])
f1=(2*precision * recall)/(precision+recall)
message("accuracy: ", accuracy, "%")
message("Recall: ", recall)
message("precision: ", precision)
message("F1: ", f1)
my_list <- list("F1" = f1, "precision" = precision, "recall" = recall, "accuracy"=accuracy)
return(my_list)
}
m.conf<-table(prolevel.test$Pro.level,tree.pred)
parse_results(m.conf)
#KNN
#Atinge 100% de accuracy sem esses dois atrbutos
#data <- data[,-c(9,2)]
prolevel.train$Pro.level <- factor(prolevel.train$Pro.level)
cv <- trainControl(method = "cv",
number = 10,
search = "grid")
tuned_model <- train(Pro.level ~ .,
data = prolevel.train,
method = "knn",
trControl = cv,
tuneGrid = expand.grid(k = 1:10))
best_k <- tuned_model$bestTune$k
final_model <- knn(train = prolevel.train,
test = prolevel.test,
cl = prolevel.train$Pro.level,
k = best_k)
confusion_matrix <- table(final_model, prolevel.test$Pro.level)
parse_results(confusion_matrix)
library(caret)
library(lubridate)
library(rpart)
library(rpart.plot)
library(rattle)
library(class)
set.seed(123)
setwd("C:/Users/manu0/Desktop/RESTO/ANADI/TP2/data")
data <- read.csv("ciclismo.csv")
#Transformando as datas em idades
ages <- interval(as.Date(data$dob), today()) %/% years(1)
data$age <- ages
#Omitindo NA's
data <- na.omit(data)
#Retirando a coluna dos Id's e das datas de nascimento
data <- data[,-c(1, 10)]
#Função de normalização
min_max <- function(data_aux) {
min_value <- min(data_aux)
max_value <- max(data_aux)
result <- (data_aux - min_value)/(max_value - min_value)
return(result)
}
#Normalização dos atributos
data$age <- min_max(data$age)
data$altitude_results <- min_max(data$altitude)
data$vo2_results <- min_max(data$vo2_results)
data$hr_results <- min_max(data$hr_results)
#Aplicando o One-Label enconder nos atributos categoricos binarios
data$gender<- ifelse(data$gender == "female", 0, 1)
data$Winter.Training.Camp<- ifelse(data$Winter.Training.Camp == "none", 0, 1)
data$Pro.level <- ifelse(data$Pro.level == "Continental", 0, 1)
#Aplicando o One-Hot Enconder nos atrbutos categoricos não binarios
encoded_df <- dummyVars("~.", data = data)
data <- data.frame(predict(encoded_df, newdata = data))
#1 - Estude a capacidade preditiva relativamente ao atributo “Pro_level”
#usando os seguintes métodos:
#divisão do dataset em treino e validação
sample = sample(c(TRUE,FALSE), nrow(data), replace = TRUE, prob = c(0.7,0.3))
prolevel.train = data[sample,]
prolevel.test = data[!sample,]
#árvore de decisão;
#A arvore de decisão tem melhores resultados sem essas colunas
#data <- data[,-c(9,2,5,8,10)]
tree.model <- rpart(Pro.level ~ . , data = prolevel.train, method = "class")
fancyRpartPlot(tree.model)
rpart.plot(tree.model, digits = 3, fallen.leaves = TRUE)
tree.pred = predict(tree.model, prolevel.test, type="class")
parse_results <- function(m.conf) {
accuracy <- 100*round((m.conf[1,1]+m.conf[2,2])/sum(m.conf),4)
recall= m.conf[1,1]/(m.conf[1,1]+m.conf[1,2])
precision = m.conf[1,1]/(m.conf[1,1]+m.conf[2,1])
f1=(2*precision * recall)/(precision+recall)
message("accuracy: ", accuracy, "%")
message("Recall: ", recall)
message("precision: ", precision)
message("F1: ", f1)
my_list <- list("F1" = f1, "precision" = precision, "recall" = recall, "accuracy"=accuracy)
return(my_list)
}
m.conf<-table(prolevel.test$Pro.level,tree.pred)
parse_results(m.conf)
#KNN
#Atinge 100% de accuracy sem esses dois atrbutos
#data <- data[,-c(9,2)]
prolevel.train$Pro.level <- factor(prolevel.train$Pro.level)
cv <- trainControl(method = "cv",
number = 10,
search = "grid")
tuned_model <- train(Pro.level ~ .,
data = prolevel.train,
method = "knn",
trControl = cv,
tuneGrid = expand.grid(k = 1:10))
best_k <- tuned_model$bestTune$k
knn.model <- knn(train = prolevel.train,
test = prolevel.test,
cl = prolevel.train$Pro.level,
k = best_k)
confusion_matrix <- table(knn.model, prolevel.test$Pro.level)
parse_results(confusion_matrix)
library(caret)
library(lubridate)
library(rpart)
library(rpart.plot)
library(rattle)
library(class)
set.seed(123)
setwd("C:/Users/manu0/Desktop/RESTO/ANADI/TP2/data")
data <- read.csv("ciclismo.csv")
#Transformando as datas em idades
ages <- interval(as.Date(data$dob), today()) %/% years(1)
data$age <- ages
#Omitindo NA's
data <- na.omit(data)
#Retirando a coluna dos Id's e das datas de nascimento
data <- data[,-c(1, 10)]
#Função de normalização
min_max <- function(data_aux) {
min_value <- min(data_aux)
max_value <- max(data_aux)
result <- (data_aux - min_value)/(max_value - min_value)
return(result)
}
#Normalização dos atributos
data$age <- min_max(data$age)
data$altitude_results <- min_max(data$altitude)
data$vo2_results <- min_max(data$vo2_results)
data$hr_results <- min_max(data$hr_results)
#Aplicando o One-Label enconder nos atributos categoricos binarios
data$gender<- ifelse(data$gender == "female", 0, 1)
data$Winter.Training.Camp<- ifelse(data$Winter.Training.Camp == "none", 0, 1)
data$Pro.level <- ifelse(data$Pro.level == "Continental", 0, 1)
#Aplicando o One-Hot Enconder nos atrbutos categoricos não binarios
encoded_df <- dummyVars("~.", data = data)
data <- data.frame(predict(encoded_df, newdata = data))
#1 - Estude a capacidade preditiva relativamente ao atributo “Pro_level”
#usando os seguintes métodos:
#divisão do dataset em treino e validação
sample = sample(c(TRUE,FALSE), nrow(data), replace = TRUE, prob = c(0.7,0.3))
prolevel.train = data[sample,]
prolevel.test = data[!sample,]
#árvore de decisão;
#A arvore de decisão tem melhores resultados sem essas colunas
#data <- data[,-c(9,2,5,8,10)]
tree.model <- rpart(Pro.level ~ . , data = prolevel.train, method = "class")
fancyRpartPlot(tree.model)
rpart.plot(tree.model, digits = 3, fallen.leaves = TRUE)
tree.pred = predict(tree.model, prolevel.test, type="class")
parse_results <- function(m.conf) {
accuracy <- 100*round((m.conf[1,1]+m.conf[2,2])/sum(m.conf),4)
recall= m.conf[1,1]/(m.conf[1,1]+m.conf[1,2])
precision = m.conf[1,1]/(m.conf[1,1]+m.conf[2,1])
f1=(2*precision * recall)/(precision+recall)
message("accuracy: ", accuracy, "%")
message("Recall: ", recall)
message("precision: ", precision)
message("F1: ", f1)
my_list <- list("F1" = f1, "precision" = precision, "recall" = recall, "accuracy"=accuracy)
return(my_list)
}
m.conf<-table(prolevel.test$Pro.level,tree.pred)
parse_results(m.conf)
#KNN
#Atinge 100% de accuracy sem esses dois atrbutos
#data <- data[,-c(9,2)]
prolevel.train$Pro.level <- factor(prolevel.train$Pro.level)
cv <- trainControl(method = "cv",
number = 10,
search = "grid")
tuned_model <- train(Pro.level ~ .,
data = prolevel.train,
method = "knn",
trControl = cv,
tuneGrid = expand.grid(k = 1:10))
best_k <- tuned_model$bestTune$k
knn.model <- knn(train = prolevel.train,
test = prolevel.test,
cl = prolevel.train$Pro.level,
k = best_k)
confusion_matrix <- table(knn.model, prolevel.test$Pro.level)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
recall <- confusion_matrix[2, 2] / (confusion_matrix[2, 2] + confusion_matrix[2, 1])
precision <- confusion_matrix[2, 2] / (confusion_matrix[2, 2] + confusion_matrix[1, 2])
f1 <- 2 * precision * recall / (precision + recall)
parse_results(confusion_matrix)
accuracy
recall
precision
f1
